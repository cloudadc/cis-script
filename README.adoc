= F5 CIS 101
:toc: manual

== LoadBalancer Type Service

[source, bash]
.*1. Install both CIS and IPAM Controller*
----
kubectl apply -f loadbalancer/install/ns.yaml 

kubectl apply -f loadbalancer/install/ipam/rbac.yaml
kubectl apply -f loadbalancer/install/ipam/localstorage-pv-pvc.yaml
kubectl apply -f loadbalancer/install/ipam/ipam_schema.yaml
kubectl apply -f loadbalancer/install/ipam/fic.yaml 

kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f loadbalancer/install/rbac.yaml 
kubectl apply -f loadbalancer/install/customresourcedefinitions.yaml 
kubectl apply -f loadbalancer/install/cis.yaml
----

Execute `kubectl get pods -n bigip-ctlr` to verify the installation.

[source, bash]
.*2. Deploy App*
----
kubectl apply -f loadbalancer/ttcp.yaml
kubectl apply -f loadbalancer/cafe.yaml 
----

== CRD

[source, bash]
.*1. Install CIS Controller*
----
kubectl apply -f crd/install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f crd/install/rbac.yaml
kubectl apply -f crd/install/customresourcedefinitions.yaml
kubectl apply -f crd/install/cis.yaml
----

[source, bash]
.*2. Deploy app*
----
kubectl apply -f crd/cafe.yaml 
kubectl apply -f crd/ttcp.yaml
----

[source, bash]
.*3. App delivery*
----
kubectl apply -f crd/vs.yaml 
kubectl apply -f crd/transport.yaml 
----

== CRD Arcadia Demo

[source, bash]
.*1. Deploy*
----
kubectl apply -f arcadia/arcadia.yaml 
----

[source, bash]
.*2. App delivery*
----
kubectl apply -f arcadia/vs.yaml 
----

== AS3 Configmap Hub Mode

[source, bash]
.*1. Install CIS Controller*
----
kubectl apply -f configmap-hub/install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f configmap-hub/install/rbac.yaml
kubectl apply -f configmap-hub/install/cis.yaml 
----

[source, bash]
.*2. Deploy App*
----
kubectl apply -f configmap-hub/apps.yaml
----

[source, bash]
.*3. Deploy Configmap*
----
kubectl apply -f configmap-hub/cm-hub-1.yaml
kubectl apply -f configmap-hub/cm-hub-2.yaml 
----

The above configmap will demostrate:

* Hub mode with 2 hub configmap in 2 namespace
* AS3 pool member's `servicePort` is optional, container exposed port is 8080, but AS3 pool member port is 80

[source, yaml]
----
              "members": [
              {
                "servicePort": 80,
----

* Immediate Action On Service Down reset mode

[source, yaml]
----
              "serviceDownImmediateAction": "reset",
----

[source, bash]
.*4. Verify deployment via tmsh*
----
~ # tmsh list auth partition | grep cistest | awk '{print $3}'
cistest1
cistest2
cistest3
cistest4
cistest5

~ # tmsh list ltm virtual /cistest3/app-1/app_svc_vs | grep reset 
    service-down-immediate-action reset
----

[source, bash]
.*5. #1387 duplicated label verification*
----
kubectl apply -f configmap-hub/duplicated-label.yaml 
----

Check from cis pod log, the following logs

[source, bash]
----
2022/05/16 08:44:11 [WARNING] [CORE] Multiple Services are tagged for this pool. Using oldest service endpoints.
Service: app-svc-1, Namespace: cistest1,Timestamp: 2022-05-16 08:07:30 +0000 UTC

2022/05/16 08:44:12 [WARNING] [CORE] Multiple Services are tagged for this pool. Using oldest service endpoints.
Service: app-svc-1, Namespace: cistest1,Timestamp: 2022-05-16 08:07:30 +0000 UTC
----

[source, bash]
.*6. Tear down*
----
kubectl delete -f configmap-hub/duplicated-label.yaml
kubectl delete -f configmap-hub/cm-hub-1.yaml
kubectl delete -f configmap-hub/cm-hub-2.yaml
kubectl delete -f configmap-hub/apps.yaml

kubectl delete -f configmap-hub/install/rbac.yaml
kubectl delete -f configmap-hub/install/cis.yaml
kubectl delete -f configmap-hub/install/ns.yaml
----

== Per Tenants Update

[source, bash]
.*1. Install CIS Controller*
----
kubectl apply -f configmap-filter-tenants/install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f configmap-filter-tenants/install/rbac.yaml
kubectl apply -f configmap-filter-tenants/install/cis.yaml
----

*2. Use the following script to test CIS control plane performance*


[cols="2,5a"]
|===
|Service Numbers |Scripts

|10
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-10.yaml 
kubectl apply -f configmap-filter-tenants/cm-10.yaml 

// add 11th app and 11th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-11.yaml 
kubectl apply -f configmap-filter-tenants/cm-11.yaml 

// upadate service, then record time 
kubectl scale -n cistest11 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-10.yaml 

// resource release
kubectl scale -n cistest11 deploy/app-1 --replicas=1
----

|20
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-20.yaml
kubectl apply -f configmap-filter-tenants/cm-20.yaml

// add 21th app and 21th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-21.yaml
kubectl apply -f configmap-filter-tenants/cm-21.yaml 

// upadate service, then record time 
kubectl scale -n cistest21 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-20.yaml

// resource release
kubectl scale -n cistest21 deploy/app-1 --replicas=1 
----

|30
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-30.yaml
kubectl apply -f configmap-filter-tenants/cm-30.yaml 

// add 31th app and 31th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-31.yaml
kubectl apply -f configmap-filter-tenants/cm-31.yaml 

// upadate service, then record time
kubectl scale -n cistest31 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-30.yaml 

// resource release
kubectl scale -n cistest31 deploy/app-1 --replicas=1
----

|40
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-40.yaml
kubectl apply -f configmap-filter-tenants/cm-40.yaml 

// add 41th app and 41th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-41.yaml
kubectl apply -f configmap-filter-tenants/cm-41.yaml

// upadate service, then record time
kubectl scale -n cistest41 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-40.yaml

// resource release
kubectl scale -n cistest41 deploy/app-1 --replicas=1 
----

|50
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-50.yaml
kubectl apply -f configmap-filter-tenants/cm-50.yaml

// add 51th app and 51th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-51.yaml
kubectl apply -f configmap-filter-tenants/cm-51.yaml

// upadate service, then record time
kubectl scale -n cistest51 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-50.yaml

// resource release
kubectl scale -n cistest51 deploy/app-1 --replicas=1
----

|60
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-60.yaml
kubectl apply -f configmap-filter-tenants/cm-60.yaml

// add 61th app and 61th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-61.yaml
kubectl apply -f configmap-filter-tenants/cm-61.yaml

// upadate service, then record time
kubectl scale -n cistest61 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-60.yaml

// resource release
kubectl scale -n cistest61 deploy/app-1 --replicas=1
----

|70
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-70.yaml
kubectl apply -f configmap-filter-tenants/cm-70.yaml

// add 71th app and 71th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-71.yaml
kubectl apply -f configmap-filter-tenants/cm-71.yaml

// upadate service, then record time
kubectl scale -n cistest71 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-70.yaml

// resource release
kubectl scale -n cistest71 deploy/app-1 --replicas=1
----

|80
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-80.yaml
kubectl apply -f configmap-filter-tenants/cm-80.yaml

// add 81th app and 81th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-81.yaml
kubectl apply -f configmap-filter-tenants/cm-81.yaml

// upadate service, then record time
kubectl scale -n cistest81 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-80.yaml

// resource release
kubectl scale -n cistest81 deploy/app-1 --replicas=1
----

|90
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-90.yaml
kubectl apply -f configmap-filter-tenants/cm-90.yaml

// add 91th app and 91th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-91.yaml
kubectl apply -f configmap-filter-tenants/cm-91.yaml

// upadate service, then record time
kubectl scale -n cistest91 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-90.yaml

// resource release
kubectl scale -n cistest91 deploy/app-1 --replicas=1
----

|100
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-100.yaml
kubectl apply -f configmap-filter-tenants/cm-100.yaml

// add 101th app and 101th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-101.yaml
kubectl apply -f configmap-filter-tenants/cm-101.yaml

// upadate service, then record time
kubectl scale -n cistest101 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-100.yaml

// resource release
kubectl scale -n cistest101 deploy/app-1 --replicas=1
----

|====

[source, bash]
.*Commands used to record time*
----
// add 11th service, then record time
STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list ltm pool /cistest11/app-1/* | grep pool | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done

// update service, then record time
STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list ltm pool /cistest11/app-1/app-1_app_svc_pool members | grep address | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done

// delete 11th service, then record time
STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list auth partition | grep cistest | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done
----

== TD

[source, bash]
.**
----

----

[source, bash]
.**
----

----

[source, bash]
.**
----

----
