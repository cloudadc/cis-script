= F5 CIS 101
:toc: manual

== LoadBalancer Type Service

[source, bash]
.*1. Install both CIS and IPAM Controller*
----
kubectl apply -f loadbalancer/install/ns.yaml 

kubectl apply -f loadbalancer/install/ipam/rbac.yaml
kubectl apply -f loadbalancer/install/ipam/localstorage-pv-pvc.yaml
kubectl apply -f loadbalancer/install/ipam/ipam_schema.yaml
kubectl apply -f loadbalancer/install/ipam/fic.yaml 

kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f loadbalancer/install/rbac.yaml 
kubectl apply -f loadbalancer/install/customresourcedefinitions.yaml 
kubectl apply -f loadbalancer/install/cis.yaml
----

Execute `kubectl get pods -n bigip-ctlr` to verify the installation.

[source, bash]
.*2. Deploy App*
----
kubectl apply -f loadbalancer/ttcp.yaml
kubectl apply -f loadbalancer/cafe.yaml 
----

== CRD

[source, bash]
.*1. Install CIS Controller*
----
kubectl apply -f crd/install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f crd/install/rbac.yaml
kubectl apply -f crd/install/customresourcedefinitions.yaml
kubectl apply -f crd/install/cis.yaml
----

[source, bash]
.*2. Deploy app*
----
kubectl apply -f crd/cafe.yaml 
kubectl apply -f crd/ttcp.yaml
----

[source, bash]
.*3. App delivery*
----
kubectl apply -f crd/vs.yaml 
kubectl apply -f crd/transport.yaml 
----

== CRD Arcadia Demo

[source, bash]
.*1. Deploy*
----
kubectl apply -f arcadia/arcadia.yaml 
----

[source, bash]
.*2. App delivery*
----
kubectl apply -f arcadia/vs.yaml 
----

== AS3 Configmap Hub Mode

[source, bash]
.*1. Install CIS Controller*
----
kubectl apply -f configmap-hub/install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f configmap-hub/install/rbac.yaml
kubectl apply -f configmap-hub/install/cis.yaml 
----

[source, bash]
.*2. Deploy App*
----
kubectl apply -f configmap-hub/apps.yaml
----

[source, bash]
.*3. Deploy Configmap*
----
kubectl apply -f configmap-hub/cm-hub-1.yaml
kubectl apply -f configmap-hub/cm-hub-2.yaml 
----

The above configmap will demostrate:

* Hub mode with 2 hub configmap in 2 namespace
* AS3 pool member's `servicePort` is optional, container exposed port is 8080, but AS3 pool member port is 80

[source, yaml]
----
              "members": [
              {
                "servicePort": 80,
----

* Immediate Action On Service Down reset mode

[source, yaml]
----
              "serviceDownImmediateAction": "reset",
----

[source, bash]
.*4. Verify deployment via tmsh*
----
~ # tmsh list auth partition | grep cistest | awk '{print $3}'
cistest1
cistest2
cistest3
cistest4
cistest5

~ # tmsh list ltm virtual /cistest3/app-1/app_svc_vs | grep reset 
    service-down-immediate-action reset
----

[source, bash]
.*5. #1387 duplicated label verification*
----
kubectl apply -f configmap-hub/duplicated-label.yaml 
----

Check from cis pod log, the following logs

[source, bash]
----
2022/05/16 08:44:11 [WARNING] [CORE] Multiple Services are tagged for this pool. Using oldest service endpoints.
Service: app-svc-1, Namespace: cistest1,Timestamp: 2022-05-16 08:07:30 +0000 UTC

2022/05/16 08:44:12 [WARNING] [CORE] Multiple Services are tagged for this pool. Using oldest service endpoints.
Service: app-svc-1, Namespace: cistest1,Timestamp: 2022-05-16 08:07:30 +0000 UTC
----

[source, bash]
.*6. Tear down*
----
kubectl delete -f configmap-hub/duplicated-label.yaml
kubectl delete -f configmap-hub/cm-hub-1.yaml
kubectl delete -f configmap-hub/cm-hub-2.yaml
kubectl delete -f configmap-hub/apps.yaml

kubectl delete -f configmap-hub/install/rbac.yaml
kubectl delete -f configmap-hub/install/cis.yaml
kubectl delete -f configmap-hub/install/ns.yaml
----

== Per Tenants Update

[source, bash]
.*1. Install CIS Controller*
----
kubectl apply -f configmap-filter-tenants/install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f configmap-filter-tenants/install/rbac.yaml
kubectl apply -f configmap-filter-tenants/install/cis.yaml
----

*2. Use the following script to test CIS control plane performance*

.*1 service per namespce*
[cols="2,5a"]
|===
|Service Numbers |Scripts

|10
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-10.yaml 
kubectl apply -f configmap-filter-tenants/cm-10.yaml 

// add 11th app and 11th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-11.yaml 
kubectl apply -f configmap-filter-tenants/cm-11.yaml 

// upadate service, then record time 
kubectl scale -n cistest011 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-10.yaml 

// resource release
kubectl scale -n cistest011 deploy/app-1 --replicas=1
----

|20
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-20.yaml
kubectl apply -f configmap-filter-tenants/cm-20.yaml

// add 21th app and 21th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-21.yaml
kubectl apply -f configmap-filter-tenants/cm-21.yaml 

// upadate service, then record time 
kubectl scale -n cistest021 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-20.yaml

// resource release
kubectl scale -n cistest021 deploy/app-1 --replicas=1 
----

|30
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-30.yaml
kubectl apply -f configmap-filter-tenants/cm-30.yaml 

// add 31th app and 31th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-31.yaml
kubectl apply -f configmap-filter-tenants/cm-31.yaml 

// upadate service, then record time
kubectl scale -n cistest031 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-30.yaml 

// resource release
kubectl scale -n cistest031 deploy/app-1 --replicas=1
----

|40
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-40.yaml
kubectl apply -f configmap-filter-tenants/cm-40.yaml 

// add 41th app and 41th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-41.yaml
kubectl apply -f configmap-filter-tenants/cm-41.yaml

// upadate service, then record time
kubectl scale -n cistest041 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-40.yaml

// resource release
kubectl scale -n cistest041 deploy/app-1 --replicas=1 
----

|50
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-50.yaml
kubectl apply -f configmap-filter-tenants/cm-50.yaml

// add 51th app and 51th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-51.yaml
kubectl apply -f configmap-filter-tenants/cm-51.yaml

// upadate service, then record time
kubectl scale -n cistest051 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-50.yaml

// resource release
kubectl scale -n cistest051 deploy/app-1 --replicas=1
----

|60
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-60.yaml
kubectl apply -f configmap-filter-tenants/cm-60.yaml

// add 61th app and 61th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-61.yaml
kubectl apply -f configmap-filter-tenants/cm-61.yaml

// upadate service, then record time
kubectl scale -n cistest061 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-60.yaml

// resource release
kubectl scale -n cistest061 deploy/app-1 --replicas=1
----

|70
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-70.yaml
kubectl apply -f configmap-filter-tenants/cm-70.yaml

// add 71th app and 71th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-71.yaml
kubectl apply -f configmap-filter-tenants/cm-71.yaml

// upadate service, then record time
kubectl scale -n cistest071 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-70.yaml

// resource release
kubectl scale -n cistest071 deploy/app-1 --replicas=1
----

|80
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-80.yaml
kubectl apply -f configmap-filter-tenants/cm-80.yaml

// add 81th app and 81th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-81.yaml
kubectl apply -f configmap-filter-tenants/cm-81.yaml

// upadate service, then record time
kubectl scale -n cistest081 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-80.yaml

// resource release
kubectl scale -n cistest081 deploy/app-1 --replicas=1
----

|90
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-90.yaml
kubectl apply -f configmap-filter-tenants/cm-90.yaml

// add 91th app and 91th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-91.yaml
kubectl apply -f configmap-filter-tenants/cm-91.yaml

// upadate service, then record time
kubectl scale -n cistest091 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-90.yaml

// resource release
kubectl scale -n cistest091 deploy/app-1 --replicas=1
----

|100
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-100.yaml
kubectl apply -f configmap-filter-tenants/cm-100.yaml

// add 101th app and 101th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-101.yaml
kubectl apply -f configmap-filter-tenants/cm-101.yaml

// upadate service, then record time
kubectl scale -n cistest101 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-100.yaml

// resource release
kubectl scale -n cistest101 deploy/app-1 --replicas=1
----

|===

.*4 services per namespce*
[cols="2,5a"]
|===
|Service Numbers |Scripts

|10
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-10-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-10-svc.yaml

// add 11th app and 11th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-11-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-11-svc.yaml

// upadate service, then record time
kubectl scale -n cistest009 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-10-svc.yaml

// resource release
kubectl scale -n cistest009 deploy/app-3 --replicas=2
----

|20
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-20-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-20-svc.yaml

// add 21th app and 21th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-21-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-21-svc.yaml

// upadate service, then record time
kubectl scale -n cistest021 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-20-svc.yaml

// resource release
kubectl scale -n cistest021 deploy/app-1 --replicas=1
----

|30
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-30-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-30-svc.yaml

// add 31th app and 31th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-31-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-31-svc.yaml

// upadate service, then record time
kubectl scale -n cistest029 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-30-svc.yaml

// resource release
kubectl scale -n cistest029 deploy/app-3 --replicas=1
----

|40
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-40-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-40-svc.yaml

// add 41th app and 41th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-41-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-41-svc.yaml

// upadate service, then record time
kubectl scale -n cistest041 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-40-svc.yaml

// resource release
kubectl scale -n cistest041 deploy/app-1 --replicas=1
----

|50
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-50-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-50-svc.yaml

// add 51th app and 51th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-51-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-51-svc.yaml

// upadate service, then record time
kubectl scale -n cistest049 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-20-svc.yaml

// resource release
kubectl scale -n cistest049 deploy/app-3 --replicas=1
----

|60
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-60-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-60-svc.yaml

// add 61th app and 61th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-61-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-61-svc.yaml

// upadate service, then record time
kubectl scale -n cistest061 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-60-svc.yaml

// resource release
kubectl scale -n cistest061 deploy/app-1 --replicas=1
----

|70
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-70-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-70-svc.yaml

// add 71th app and 71th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-71-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-71-svc.yaml

// upadate service, then record time
kubectl scale -n cistest069 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-70-svc.yaml

// resource release
kubectl scale -n cistest069 deploy/app-3 --replicas=1
----

|80
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-80-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-80-svc.yaml

// add 81th app and 81th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-81-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-81-svc.yaml

// upadate service, then record time
kubectl scale -n cistest081 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-80-svc.yaml

// resource release
kubectl scale -n cistest081 deploy/app-1 --replicas=1
----

|90
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-90-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-90-svc.yaml

// add 91th app and 91th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-91-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-91-svc.yaml

// upadate service, then record time
kubectl scale -n cistest089 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-90-svc.yaml

// resource release
kubectl scale -n cistest089 deploy/app-3 --replicas=1
----

|100
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-100-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-100-svc.yaml

// add 101th app and 101th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-101-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-101-svc.yaml

// upadate service, then record time
kubectl scale -n cistest101 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-100-svc.yaml

// resource release
kubectl scale -n cistest101 deploy/app-1 --replicas=1
----

|110
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-110-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-110-svc.yaml

// add 111th app and 111th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-111-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-111-svc.yaml

// upadate service, then record time
kubectl scale -n cistest109 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-110-svc.yaml

// resource release
kubectl scale -n cistest109 deploy/app-3 --replicas=1
----

|120
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-120-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-120-svc.yaml

// add 111th app and 111th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-121-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-121-svc.yaml

// upadate service, then record time
kubectl scale -n cistest121 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-120-svc.yaml

// resource release
kubectl scale -n cistest121 deploy/app-1 --replicas=1
----

|130
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-130-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-130-svc.yaml

// add 131th app and 131th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-131-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-131-svc.yaml

// upadate service, then record time
kubectl scale -n cistest129 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-130-svc.yaml

// resource release
kubectl scale -n cistest129 deploy/app-3 --replicas=1
----

|140
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-140-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-140-svc.yaml

// add 141th app and 141th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-141-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-141-svc.yaml

// upadate service, then record time
kubectl scale -n cistest141 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-140-svc.yaml

// resource release
kubectl scale -n cistest141 deploy/app-1 --replicas=1
----

|150
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-150-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-150-svc.yaml

// add 151th app and 151th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-151-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-151-svc.yaml

// upadate service, then record time
kubectl scale -n cistest149 deploy/app-3 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-150-svc.yaml

// resource release
kubectl scale -n cistest149 deploy/app-3 --replicas=1
----

|160
|

[source, bash]
----
kubectl apply -f configmap-filter-tenants/deploy-160-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-160-svc.yaml

// add 161th app and 161th vs and record time spended
kubectl apply -f configmap-filter-tenants/deploy-161-svc.yaml
kubectl apply -f configmap-filter-tenants/cm-161-svc.yaml

// upadate service, then record time
kubectl scale -n cistest161 deploy/app-1 --replicas=2

// delete service from BIG-IP, record time
kubectl apply -f configmap-filter-tenants/cm-160-svc.yaml

// resource release
kubectl scale -n cistest161 deploy/app-1 --replicas=1
----
|===

[source, bash]
.*Commands used to record time*
----
// add 11th service, then record time
STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list ltm pool /cistest011/app-1/* | grep pool | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done

// update service, then record time
STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list ltm pool /cistest011/app-1/app-1_app_svc_pool members | grep address | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done

// delete 11th service, then record time
STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list auth partition | grep cistest | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done

# cat ./record_pool.sh 
#!/bin/bash

STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list ltm pool /cistest$1/app-$2/* | grep pool | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done

# cat ./record_member.sh 
#!/bin/bash

STARTTIME=$(date +%s) ; for i in {1..100} ; do tmsh list ltm pool /cistest$1/app-$2/app-$3_app_svc_pool members | grep address | wc -l ; ENDTIME=$(date +%s); echo "spend $(($ENDTIME - $STARTTIME)) seconds" ; sleep 3 ; done
----

[source, bash]
.*Simulate a cluster offline*
----
for i in $(kubectl get ns | grep cistest | awk '{print $1}') ; do for j in $(kubectl get pods -n $i --no-headers | awk '{print $1}') ; do kubectl delete pod $j -n $i ; done ; done;
----

[source, bash]
.**
----

----

[source, bash]
.**
----

----
