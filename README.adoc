= F5 CIS 101
:toc: manual

== 安装

说明：

* livenessProbe 探测 BIG-IP
* 最小化 RBAC（读写分开，读 cluster wide，写 namespace wide）
* 启用hub模式
* 启用基于租户增量更新

[source, bash]
.*安装*
----
kubectl apply -f install/ns.yaml
kubectl create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n bigip-ctlr
kubectl apply -f install/rbac-small.yaml
kubectl apply -f install/cis.yaml 
----

[source, bash]
.*移除*
----
kubectl delete -f install/rbac.yaml
kubectl delete -f install/cis.yaml
kubectl delete -f install/ns.yaml
----

== 功能测试

=== 会话保持

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 001/app.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 001/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ curl http://192.168.200.31 -v
...
< Set-Cookie: BIGipServer~cistest001~app-1~app_1_svc_pool=2400272484.36895.0000; path=/; Httponly
< 
Server address: 100.64.17.143:8080
Server name: app-1-7f4585dc79-5xkqs
Date: 19/Jul/2022:11:47:36 +0000
URI: /
Request ID: 9d6a255a5216a891e8588b1395ec8bf5

$ curl --cookie "BIGipServer~cistest001~app-1~app_1_svc_pool=2400272484.36895.0000" http://192.168.200.31/test
Server address: 100.64.17.143:8080
Server name: app-1-7f4585dc79-5xkqs
Date: 19/Jul/2022:11:50:26 +0000
URI: /test
Request ID: b92bbb845b76bc4348c1d4a3d3e422eb

$ curl --cookie "BIGipServer~cistest001~app-1~app_1_svc_pool=2400272484.36895.0000" http://192.168.200.31/test/user
Server address: 100.64.17.143:8080
Server name: app-1-7f4585dc79-5xkqs
Date: 19/Jul/2022:11:50:36 +0000
URI: /test/user
Request ID: d4fbc16fea0300ad67263a376a5eab38
----

=== Cookie 加密 

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 002/app.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 002/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ curl http://192.168.200.32 -v
...
< Set-Cookie: BIGipServer~cistest002~app-1~app_1_svc_pool=!5agmNHYLuqqe3qfKX3XmY+C0N2Z48JQp+ps7BHHI7cFyhqrRVC/WhN3goMDCQf/nBpJ8+qCR5uT7Slg=; path=/; Httponly
< 
Server address: 100.64.21.180:8080
Server name: app-1-7f4585dc79-n2k6z
Date: 19/Jul/2022:11:59:59 +0000
URI: /
Request ID: c4f8480f1b7ee744c33ccff729f8c99a

$ curl --cookie 'BIGipServer~cistest002~app-1~app_1_svc_pool=!iQ5xKJ7r5J5cx47KX3XmY+C0N2Z48EzgRDLD6LmcMmk5aIzT+IdWNWeMolr/H7KhlzScsmiZMkuQ25o=' http://192.168.200.32/test
Server address: 100.64.21.180:8080
Server name: app-1-7f4585dc79-n2k6z
Date: 19/Jul/2022:12:00:07 +0000
URI: /test
Request ID: 728c77ad635347ec83ef12c993dd54d1

$ curl --cookie 'BIGipServer~cistest002~app-1~app_1_svc_pool=!iQ5xKJ7r5J5cx47KX3XmY+C0N2Z48EzgRDLD6LmcMmk5aIzT+IdWNWeMolr/H7KhlzScsmiZMkuQ25o=' http://192.168.200.32/test/user
Server address: 100.64.21.180:8080
Server name: app-1-7f4585dc79-n2k6z
Date: 19/Jul/2022:12:00:10 +0000
URI: /test/user
Request ID: 6a4cfaec2d62011848adb982415fc388
----

=== 根据请求路径和返回值判断的健康检查

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 003/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 003/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest003/app-1/app_1_svc_pool monitor 
Password: 
ltm pool /cistest003/app-1/app_1_svc_pool {
    monitor min 1 of { /cistest003/app-1/custom_http_monitor }
}
----

=== 多健康检查（TCP + HTTP）

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 004/app.yaml 
----

[source, bash]
.*2. 下发服务（仅 TCP）*
----
kubectl apply -f 004/cm.1.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest004/app-1/app_1_svc_pool monitor 
Password: 
ltm pool /cistest004/app-1/app_1_svc_pool {
    monitor min 1 of { tcp }
}
----

[source, bash]
.*4. 下发服务（TCP + HTTP）*
----
kubectl apply -f 004/cm.2.yaml
----

[source, bash]
.*5. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest004/app-1/app_1_svc_pool monitor 
Password: 
ltm pool /cistest004/app-1/app_1_svc_pool {
    monitor min 1 of { tcp /cistest004/app-1/custom_http_monitor }
}
----

[source, bash]
.*6. 下发服务（HTTP）*
----
kubectl apply -f 004/cm.3.yaml
----

[source, bash]
.*7. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest004/app-1/app_1_svc_pool monitor
Password:
ltm pool /cistest004/app-1/app_1_svc_pool {
    monitor min 1 of { http }
}
----

[source, bash]
.*8. 下发服务（TCP + HTTP）*
----
kubectl apply -f 004/cm.2.yaml
----

[source, bash]
.*9. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest004/app-1/app_1_svc_pool monitor
Password:
ltm pool /cistest004/app-1/app_1_svc_pool {
    monitor min 1 of { tcp /cistest004/app-1/custom_http_monitor }
}
----

=== 使用 Rule 控制流量

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 005/app-1.yaml 
kubectl apply -f 005/app-2.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 005/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm rule /cistest005/app-1/iRulesHere
Password: 
ltm rule /cistest005/app-1/iRulesHere {
    partition cistest005
when HTTP_REQUEST {
 if { [HTTP::uri] contains "foo" } {
   pool /cistest005/app-1/app_1_svc_pool
 } elseif {[HTTP::uri] contains "bar"} {
   pool /cistest005/app-2/app_2_svc_pool
 } else {
 pool   /cistest005/app-1/app_1_svc_pool
 }
}
}
----

=== 使用 Policy 控制流量 

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 006/app-1.yaml
kubectl apply -f 006/app-2.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 006/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm policy /cistest006/app/forward_policy
Password: 
ltm policy /cistest006/app/forward_policy {
    controls { forwarding }
    last-modified 2022-07-19:22:03:04
    partition cistest006
    requires { http }
    rules {
        forward_to_poo1 {
            actions {
                0 {
                    forward
                    select
                    pool /cistest006/app/app_1_svc_pool
                }
            }
            conditions {
                0 {
                    http-uri
                    path
                    contains
                    values { foo }
                }
            }
        }
        forward_to_poo2 {
            actions {
                0 {
                    forward
                    select
                    pool /cistest006/app/app_2_svc_pool
                }
            }
            conditions {
                0 {
                    http-uri
                    path
                    contains
                    values { bar }
                }
            }
            ordinal 1
        }
    }
    status legacy
    strategy best-match
}
----

NOTE: Rule 可以跨 partion, 跨 app，Policy 必需在同一个 app 中。

=== 源地址会话保持

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 007/app.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 007/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ for i in {1..5} ; do curl -s http://192.168.200.37 | grep address | awk '{print $3}' ; done
100.64.21.158:8080
100.64.21.158:8080
100.64.21.158:8080
100.64.21.158:8080
100.64.21.158:8080
----

=== 启用连接镜像

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 008/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 008/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm virtual /cistest008/app-1/app_svc_vs mirror
Password: 
ltm virtual /cistest008/app-1/app_svc_vs {
    mirror enabled
}
----

=== 负载算法

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 009/app.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 009/cm-1.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest009/app-1/app_1_svc_pool load-balancing-mode
Password: 
ltm pool /cistest009/app-1/app_1_svc_pool {
    load-balancing-mode least-connections-member
}
----

[source, bash]
.*4. 下发服务*
----
kubectl apply -f 009/cm-3.yaml 
----

[source, bash]
.*5. 测试验证*
----
BEI-ML-00005336:cis-scripts ksong$ ssh root@192.168.200.204 tmsh list ltm pool /cistest009/app-1/app_1_svc_pool load-balancing-mode
Password: 
ltm pool /cistest009/app-1/app_1_svc_pool {
    load-balancing-mode round-robin
}
----

[source, bash]
.*6. 下发服务*
----
kubectl apply -f 009/cm-3.yaml
----

[source, bash]
.*7. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm pool /cistest009/app-1/app_1_svc_pool load-balancing-mode
Password: 
ltm pool /cistest009/app-1/app_1_svc_pool {
    load-balancing-mode least-sessions
}
----

=== SNAT Pool 支持 

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 010/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 010/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm snatpool /cistest010/app-1/app_svc_vs-self
Password: 
ltm snatpool /cistest010/app-1/app_svc_vs-self {
    members {
        /cistest010/app-1/192.168.200.40
    }
    partition cistest010
}
----

=== 自定义 TCP Profile

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 011/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 011/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm profile tcp /cistest011/app-1/customTCPProfile idle-timeout
Password: 
ltm profile tcp /cistest011/app-1/customTCPProfile {
    idle-timeout 600
}

----

=== 自定义 HTTP Profile

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 012/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 012/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm profile http /cistest012/app-1/customHTTPProfile insert-xforwarded-for
ltm profile http /cistest012/app-1/customHTTPProfile {
    insert-xforwarded-for enabled
}
----

=== 自定义 OneConnect Profile

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 013/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 013/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm profile one-connect /cistest013/app-1/customOneConnectProfile
Password: 
ltm profile one-connect /cistest013/app-1/customOneConnectProfile {
    app-service none
    description none
    idle-timeout-override disabled
    limit-type none
    max-age 86400
    max-reuse 1000
    max-size 10000
    share-pools disabled
    source-mask 255.255.255.255
}
----

=== 自定义 SSL Profile

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 014/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 014/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ curl https://192.168.200.44 -k -v
*   Trying 192.168.200.44...
* TCP_NODELAY set
* Connected to 192.168.200.44 (192.168.200.44) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/cert.pem
  CApath: none
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server did not agree to a protocol
* Server certificate:
*  subject: C=US; ST=WA; L=Seattle; O=MyCompany; OU=IT; CN=localhost.localdomain; emailAddress=root@localhost.localdomain
*  start date: Apr 15 06:24:16 2021 GMT
*  expire date: Apr 13 06:24:16 2031 GMT
*  issuer: C=US; ST=WA; L=Seattle; O=MyCompany; OU=IT; CN=localhost.localdomain; emailAddress=root@localhost.localdomain
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
> GET / HTTP/1.1
> Host: 192.168.200.44
> User-Agent: curl/7.64.1
> Accept: */*
> 
< HTTP/1.1 200 OK
< Server: nginx/1.16.1
< Date: Sun, 24 Jul 2022 07:40:27 GMT
< Content-Type: text/plain
< Content-Length: 155
< Connection: keep-alive
< Expires: Sun, 24 Jul 2022 07:40:26 GMT
< Cache-Control: no-cache
< Set-Cookie: BIGipServer~cistest014~app-1~app_1_svc_pool=504840292.36895.0000; path=/; Httponly; Secure
< 
Server address: 100.64.23.30:8080
Server name: app-1-7f4585dc79-6xc2n
Date: 24/Jul/2022:07:40:27 +0000
URI: /
Request ID: 0473c17d40cde2901ebe300ce3b87658

----

=== 四层透传模式

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 015/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 015/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list ltm virtual /cistest015/app-1/app_svc_vs
Password: 
ltm virtual /cistest015/app-1/app_svc_vs {
    creation-time 2022-07-24:15:20:17
    description app-1
    destination /cistest015/192.168.200.45:http
    last-modified-time 2022-07-24:15:20:17
    mask 255.255.255.255
    partition cistest015
    persist {
        source_addr {
            default yes
        }
    }
    pool /cistest015/app-1/app_1_svc_pool
    profiles {
        fastL4 { }
    }
    serverssl-use-sni disabled
    source 0.0.0.0/0
    source-address-translation {
        pool /cistest015/app-1/app_svc_vs-self
        type snat
    }
    translate-address enabled
    translate-port enabled
    vs-index 2928
}
----

=== VS 状态与 pool-member 状态联动 Down

[source, bash]
.*1. 发布服务*
----
kubectl apply -f 016/cm-1.yaml
----

[source, bash]

.*2. Telnet 测试(尽管 VS 为红色，但 Telnet 成功)*
----
$ telnet 192.168.200.46 80
Trying 192.168.200.46...
Connected to 192.168.200.46.
Escape character is '^]'.
----

[source, bash]
.*3. 发布服务，启用service Down Immediate Action*
----
kubectl apply -f 016/cm-2.yaml 
----

[source, bash]
.*4. Telnet 测试*
----
$ telnet 192.168.200.46 80
Trying 192.168.200.46...
Connected to 192.168.200.46.
Escape character is '^]'.
Connection closed by foreign host.
----

=== AS3 ServicePort 配置可选

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 017/app.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 017/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ curl http://192.168.200.47 -I
HTTP/1.1 200 OK
Server: nginx/1.16.1
Date: Sun, 24 Jul 2022 07:41:14 GMT
Content-Type: text/plain
Content-Length: 156
Connection: keep-alive
Expires: Sun, 24 Jul 2022 07:41:13 GMT
Cache-Control: no-cache
Set-Cookie: BIGipServer~cistest017~app-1~app_1_svc_pool=2148876388.36895.0000; path=/; Httponly
----

=== 不同 service 拥有相同 labels

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 018/app.yaml
----

[source, bash]
.*2. 测试验证*
----
// check the cis log
2022/07/23 15:53:59 [WARNING] [CORE] Multiple Services are tagged for this pool. Using oldest service endpoints.
Service: app-svc-1, Namespace: cistest001,Timestamp: 2022-07-23 07:50:52 +0000 UTC

// verify the service on cistest001
$ curl http://192.168.200.31 -I
HTTP/1.1 200 OK
Server: nginx/1.16.1
Date: Sat, 23 Jul 2022 15:55:09 GMT
Content-Type: text/plain
Content-Length: 155
Connection: keep-alive
Expires: Sat, 23 Jul 2022 15:55:08 GMT
Cache-Control: no-cache
Set-Cookie: BIGipServer~cistest001~app-1~app_1_svc_pool=857161828.36895.0000; path=/; Httponly
----

== 场景测试

=== 灰度发布

[source, bash]
.*1. 部署应用(Deploy 2 version of app, 1.0 version on test001, 1.1 version on test002)*
----
kubectl apply -f 101/backend-canary.yaml
----

*2. 六种灰度发布*

[cols="2,5a"]
|===
|Methods |Steps

|URL
|Deploy

----
kubectl apply -f 101/cm-canary-v1.yaml
kubectl apply -f 101/cm-canary-v2.yaml
kubectl apply -f 101/cm-canary-url.yaml
----

Test

----
curl 192.168.200.11/foo
----

|URL Parameter
|Deploy

----
kubectl apply -f 101/cm-canary-v1.yaml
kubectl apply -f 101/cm-canary-v2.yaml
kubectl apply -f 101/cm-canary-parametes.yaml
----

Test

----
curl 192.168.200.11/foo?name=1010
----

|Source Address
|Deploy

----
kubectl apply -f 101/cm-canary-v1.yaml
kubectl apply -f 101/cm-canary-v2.yaml
kubectl apply -f 101/cm-canary-sourceaddr.yaml
----

Test

----
curl 192.168.200.11/foo
----

|Http Header
|Deploy

----
kubectl apply -f 101/cm-canary-v1.yaml
kubectl apply -f 101/cm-canary-v2.yaml
kubectl apply -f 101/cm-canary-headers.yaml
----

Test

----
curl 192.168.200.11/foo --header "Canary: true"
----

|Cookie
|Deploy

----
kubectl apply -f 101/cm-canary-v1.yaml
kubectl apply -f 101/cm-canary-v2.yaml
kubectl apply -f 101/cm-canary-cookie.yaml
----

Test

----
curl 192.168.200.11/foo --cookie "Canary=true"
----

|Ratio
|Deploy

----
kubectl apply -f 101/cm-canary-v1.yaml
kubectl apply -f 101/cm-canary-v2.yaml
kubectl apply -f 101/cm-canary-ratio.yaml
----

Test

----
curl 192.168.200.11/foo
----

|===

=== 根据路径转发 

[source, bash]
.*1. 部署应用*
----
kubectl apply -f 102/apps.yaml 
----

[source, bash]
.*2. 发布服务*
----
kubectl apply -f 102/cm-v1.yaml
kubectl apply -f 102/cm-v2.yaml 
----

[source, bash]
.*3. 查看转发规则*
----
when HTTP_REQUEST {
  if { [HTTP::uri] starts_with "/api" } {
    pool /test003/api/api-svc-pool
  } elseif { [HTTP::uri] starts_with "/files" } {
    pool /test003/backend/backend-svc-pool
  } elseif { [HTTP::uri] starts_with "/app3" } {
    pool /test003/refer/refer-svc-pool
  } else {
    pool /test003/main/main-svc-pool
  }
}
----

== 架构测试

=== Hub 模式 - 应用和应用描述文件Configmap 在不同的 NS

一个 CIS 监控两个 NS，每个 NS 下一个 Configmap, 第一个 Configmap 发布 3 个服务，第二个 Configmap 发布 5个服务。

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 201/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 201/cm-hub-1.yaml
kubectl apply -f 201/cm-hub-2.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 'for i in {1..5}; do tmsh list ltm virtual /cistest$i/app-1/app_svc_vs one-line ; done'
Password: 
ltm virtual /cistest1/app-1/app_svc_vs { creation-time 2022-07-24:22:21:38 description app-1 destination /cistest1/10.1.10.1:http ip-protocol tcp last-modified-time 2022-07-24:22:21:38 mask 255.255.255.255 partition cistest1 persist { cookie { default yes } } pool /cistest1/app-1/app-1_app_svc_pool profiles { f5-tcp-progressive { } http { } } serverssl-use-sni disabled source 0.0.0.0/0 source-address-translation { pool /cistest1/app-1/app_svc_vs-self type snat } translate-address enabled translate-port enabled vs-index 2935 }
ltm virtual /cistest2/app-1/app_svc_vs { creation-time 2022-07-24:22:21:04 description app-1 destination /cistest2/10.1.10.2:http ip-protocol tcp last-modified-time 2022-07-24:22:21:04 mask 255.255.255.255 partition cistest2 persist { cookie { default yes } } pool /cistest2/app-1/app-1_app_svc_pool profiles { f5-tcp-progressive { } http { } } serverssl-use-sni disabled source 0.0.0.0/0 source-address-translation { pool /cistest2/app-1/app_svc_vs-self type snat } translate-address enabled translate-port enabled vs-index 2933 }
ltm virtual /cistest3/app-1/app_svc_vs { creation-time 2022-07-24:22:22:22 description app-1 destination /cistest3/10.1.10.3:http ip-protocol tcp last-modified-time 2022-07-24:22:22:22 mask 255.255.255.255 partition cistest3 persist { cookie { default yes } } pool /cistest3/app-1/app-1_app_svc_pool profiles { f5-tcp-progressive { } http { } } serverssl-use-sni disabled service-down-immediate-action reset source 0.0.0.0/0 source-address-translation { pool /cistest3/app-1/app_svc_vs-self type snat } translate-address enabled translate-port enabled vs-index 2937 }
ltm virtual /cistest4/app-1/app_svc_vs { creation-time 2022-07-24:22:22:04 description app-1 destination /cistest4/10.1.10.4:http ip-protocol tcp last-modified-time 2022-07-24:22:22:04 mask 255.255.255.255 partition cistest4 persist { cookie { default yes } } pool /cistest4/app-1/app-1_app_svc_pool profiles { f5-tcp-progressive { } http { } } serverssl-use-sni disabled source 0.0.0.0/0 source-address-translation { pool /cistest4/app-1/app_svc_vs-self type snat } translate-address enabled translate-port enabled vs-index 2936 }
ltm virtual /cistest5/app-1/app_svc_vs { creation-time 2022-07-24:22:21:20 description app-1 destination /cistest5/10.1.10.5:http ip-protocol tcp last-modified-time 2022-07-24:22:21:20 mask 255.255.255.255 partition cistest5 persist { cookie { default yes } } pool /cistest5/app-1/app-1_app_svc_pool profiles { f5-tcp-progressive { } http { } } serverssl-use-sni disabled source 0.0.0.0/0 source-address-translation { pool /cistest5/app-1/app_svc_vs-self type snat } translate-address enabled translate-port enabled vs-index 2934 }
----

=== 一个 NS 下有多个 Configmap

一个 NS 下通过多个 Configmap 发布服务

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 202/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 202/cm-202-a.yaml 
kubectl apply -f 202/cm-202-b.yaml 
----

[source, bash]
.*3. 测试验证*
----
// query configmap from hub-1
$ kubectl get cm -n f5-hub-1 | grep 202
cm-202-a     1      7m30s
cm-202-b     1      5m19s

// echo from BIG-IP
$ ssh root@192.168.200.204 'tmsh list ltm virtual /cistest6/app-1/app_svc_vs ; echo ; tmsh list ltm virtual /cistest7/app-1/app_svc_vs'
Password: 
ltm virtual /cistest6/app-1/app_svc_vs {
    creation-time 2022-07-24:22:35:06
    description app-1
    destination /cistest6/10.1.10.6:http
    ip-protocol tcp
    last-modified-time 2022-07-24:22:35:06
    mask 255.255.255.255
    partition cistest6
    persist {
        cookie {
            default yes
        }
    }
    pool /cistest6/app-1/app-1_app_svc_pool
    profiles {
        f5-tcp-progressive { }
        http { }
    }
    serverssl-use-sni disabled
    source 0.0.0.0/0
    source-address-translation {
        pool /cistest6/app-1/app_svc_vs-self
        type snat
    }
    translate-address enabled
    translate-port enabled
    vs-index 2938
}

ltm virtual /cistest7/app-1/app_svc_vs {
    creation-time 2022-07-24:22:37:12
    description app-1
    destination /cistest7/10.1.10.7:http
    ip-protocol tcp
    last-modified-time 2022-07-24:22:37:12
    mask 255.255.255.255
    partition cistest7
    persist {
        cookie {
            default yes
        }
    }
    pool /cistest7/app-1/app-1_app_svc_pool
    profiles {
        f5-tcp-progressive { }
        http { }
    }
    serverssl-use-sni disabled
    source 0.0.0.0/0
    source-address-translation {
        pool /cistest7/app-1/app_svc_vs-self
        type snat
    }
    translate-address enabled
    translate-port enabled
    vs-index 2939
}
----

=== 一个 K8S NS 对应多个 BIG-IP Partition

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 203/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 203/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 tmsh list auth partition | grep cistest8 | awk '{print $3}'
Password: 
cistest8-1
cistest8-2
----

=== 一个 K8S NS 内的多个 SVC 对应 BIG-IP 上一个 Partition 内多个 APP

K8S SVC 和 BIG-IP APP 1 对 1 关系。 

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 204/apps.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 204/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 'for i in 1 2 ; do tmsh list ltm virtual /cistest9/app-$i/app_svc_vs | grep virtual ; done' 
Password: 
ltm virtual /cistest9/app-1/app_svc_vs {
ltm virtual /cistest9/app-2/app_svc_vs {
----

=== 一个 K8S NS 内的多个 SVC 对应 BIG-IP 上一个 Partition 内一个 APP

K8S SVC 和 BIG-IP APP 多对 1 关系。

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 205/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 205/cm.yaml
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 'tmsh list ltm virtual /cistest10/app/app_svc_1_vs | grep virtual ; tmsh list ltm virtual /cistest10/app/app_svc_2_vs | grep virtual'
Password: 
ltm virtual /cistest10/app/app_svc_1_vs {
ltm virtual /cistest10/app/app_svc_2_vs {
----

=== 一个 K8S SVC 对应多个 BIG-IP VS（同租户共享 APP 实现）

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 206/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 206/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 'tmsh list ltm virtual /cistest11/cistest11_1/app_svc_vs pool ; tmsh list ltm virtual /cistest11/cistest11_2/app_svc_vs pool'
Password: 
ltm virtual /cistest11/cistest11_1/app_svc_vs {
    pool /cistest11/Shared/app_svc_pool
}
ltm virtual /cistest11/cistest11_2/app_svc_vs {
    pool /cistest11/Shared/app_svc_pool
}
----

=== 一个 K8S SVC 对应多个 BIG-IP VS（同租户下共享 Node）

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 207/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 207/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 'tmsh list ltm pool /cistest12/app/app-svc-1_pool members | grep address ;tmsh list ltm pool /cistest12/app/app-svc-2_pool members | grep address'
Password: 
            address 100.64.23.28
            address 100.64.23.28
----

NOTE: 只适用于 http 模板。

=== 一个 K8S SVC 对应多个 BIG-IP VS（/Common 下共享 Node）

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 208/apps.yaml 
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 208/cm.yaml 
----

[source, bash]
.*3. 测试验证*
----
$ ssh root@192.168.200.204 'tmsh list ltm pool /cistest13-1/app/app-svc-1_pool ; tmsh list ltm pool /cistest13-2/app/app-svc-2_pool ; tmsh list ltm node 100.64.23.12 '
Password: 
ltm pool /cistest13-1/app/app-svc-1_pool {
    load-balancing-mode least-connections-member
    members {
        100.64.23.12:webcache {
            address 100.64.23.12
            session monitor-enabled
            state up
            metadata {
                source {
                    value declaration
                }
            }
        }
    }
    min-active-members 1
    monitor min 1 of { tcp }
    partition cistest13-1
}
ltm pool /cistest13-2/app/app-svc-2_pool {
    load-balancing-mode least-connections-member
    members {
        100.64.23.12:webcache {
            address 100.64.23.12
            session monitor-enabled
            state up
            metadata {
                source {
                    value declaration
                }
            }
        }
    }
    min-active-members 1
    monitor min 1 of { tcp }
    partition cistest13-2
}
ltm node 100.64.23.12 {
    address 100.64.23.12
    metadata {
        references {
            value 2
        }
    }
}
----

=== 管理平面隔离性 - 一个应用的下发失败不影响另一个应用的下发 

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 209/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 209/cm-1.yaml
kubectl apply -f 209/cm-2.yaml 
kubectl apply -f 209/cm-3.yaml
----

[source, bash]
.*3. 测试验证*
----
// check from cis log
2022/07/25 14:41:33 [ERROR] [AS3][Configmap] Error validating AS3 template
2022/07/25 14:41:33 [ERROR] [AS3][Configmap] Error in processing the resource ConfigMap: cm-209-1 in Namespace: f5-hub-1

// verify the deployed vs
$ ssh root@192.168.200.204 'tmsh list ltm virtual /cistest15/app/app_svc_1_vs pool ; tmsh list ltm virtual /cistest16/app/app_svc_1_vs pool'
Password: 
ltm virtual /cistest15/app/app_svc_1_vs {
    pool /cistest15/app/app-1_app_svc_pool
}
ltm virtual /cistest16/app/app_svc_1_vs {
    pool /cistest16/app/app-1_app_svc_pool
}
----

=== BIG-IP 宕机后下发容错能力

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 210/apps.yaml
----

[source, bash]
.*2. 查看 CIS POD 内容器 RESTARTS 为 0*
----
$ kubectl get pods -n bigip-ctlr
NAME                                          READY   STATUS    RESTARTS   AGE
bigip-ctlr-192-168-200-204-565c7d6549-hbghn   1/1     Running   0          11h
----

*3. 重启 BIG-IP*

[source, bash]
.*4. 下发服务*
----
for i in 1 2 3 ; do kubectl apply -f 210/cm-$i.yaml ; sleep 60 ; done
----

[source, bash]
.*5. 查看 CIS POD 内容器 RESTARTS 为 6*
----
$ kubectl get pods -n bigip-ctlr
NAME                                          READY   STATUS    RESTARTS   AGE
bigip-ctlr-192-168-200-204-565c7d6549-hbghn   1/1     Running   6          12h

$ ns=bigip-ctlr; p=$(kubectl get pods -n $ns --no-headers | awk '{print $1}'); kubectl describe pods $p -n $ns
...
    State:          Running
      Started:      Tue, 26 Jul 2022 09:19:16 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 26 Jul 2022 09:17:50 +0800
      Finished:     Tue, 26 Jul 2022 09:17:51 +0800
    Ready:          True
    Restart Count:  6
    Liveness:       exec [curl -k -s -o /dev/null https://192.168.200.204/mgmt/shared/appsvcs/info] delay=15s timeout=5s period=15s #success=1 #failure=3
...
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  35m (x3 over 35m)  kubelet  Liveness probe failed:
  Normal   Killing    35m                kubelet  Container bigip-ctlr failed liveness probe, will be restarted
----

[source, bash]
.*6. 测试验证*
----
$ ssh root@192.168.200.204 'for i in 17 18 19 ; do tmsh list ltm virtual /cistest$i/app/app_svc_1_vs destination ; done'
Password: 
ltm virtual /cistest17/app/app_svc_1_vs {
    destination /cistest17/10.1.10.23:http
}
ltm virtual /cistest18/app/app_svc_1_vs {
    destination /cistest18/10.1.10.24:http
}
ltm virtual /cistest19/app/app_svc_1_vs {
    destination /cistest19/10.1.10.25:http
}
----

=== 自动化监控日志代码汇总

[source, bash]
.*1. 创建应用*
----
kubectl apply -f 211/apps.yaml
----

[source, bash]
.*2. 下发服务*
----
kubectl apply -f 211/cm.yaml
----

*3. 日志告警及代码*

[cols="2,5a"]
|===
|类型 | 日志

|VS 
|类型 | 日志

|VS 地址冲突
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:19:20 [ERROR] [AS3] Big-IP Responded with error code: 422
2022/07/26 02:19:20 [ERROR] [AS3] Raw response from Big-IP: map[code:422 declaration:map[class:ADC controls:map[archiveTimestamp:2022-07-26T01:54:10.103Z class:Controls userAgent:CIS Configured AS3] id:urn:uuid:85626792-9ee7-46bb-8fc8-4ba708cfdc1d label:CIS Declaration remark:Auto-generated by CIS schemaVersion:3.36.0 updateMode:selective] results:[map[code:422 host:localhost message:declaration failed response:0107176c:3: Invalid Virtual Address, the IP address 10.1.10.25 already exists. runTime:2106 tenant:cistest211]]] 
----

[source, bash]
.*restnoded.log*
----
Tue, 26 Jul 2022 01:54:10 GMT - severe: [appsvcs] {"message":"Declaration failed: 0107176c:3: Invalid Virtual Address, the IP address 10.1.10.25 already exists.","level":"error"}
----

[source, bash]
.*LTM 日志*
----
Jul 26 09:54:09 bigip1.com err mcpd[7242]: 0107176c:3: Invalid Virtual Address, the IP address 10.1.10.25 already exists.
----

|Member 地址冲突
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:26:53 [ERROR] [AS3] Big-IP Responded with error code: 422
2022/07/26 02:26:53 [ERROR] [AS3] Raw response from Big-IP: map[code:422 declarationFullId: errors:[/cistest211/app/app_svc_pool/members: pool member /cistest211/app/app_svc_pool/members/0 static address 100.64.23.37 conflicts with bigip node /cistest011/100.64.23.37] message:declaration is invalid] 
----

[source, bash]
.*restnoded.log*
----
Tue, 26 Jul 2022 02:01:55 GMT - warning: [appsvcs] {"status":422,"message":"declaration is invalid","errors":["/cistest211/app/app_svc_pool/members: pool member /cistest211/app/app_svc_pool/members/0 static address 100.64.23.37 conflicts with bigip node /cistest011/100.64.23.37"],"level":"warning"}
----

|会话保持配置错误
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:34:41 [ERROR] - declaration.persistenceMethods.0: declaration.persistenceMethods.0 must be one of the following: "cookie", "destination-address", "msrdp", "source-address", "tls-session-id"
----

|VS 端口错误
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:36:10 [ERROR] Error processing configmap cm-211 in namespace: f5-hub-1 with err: invalid character 'o' after object key:value pair
----

|健康检查配置错误
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:38:25 [ERROR] - declaration.monitors.0: declaration.monitors.0 must be one of the following: "http", "https", "http2", "icmp", "tcp-half-open", "tcp"
----

|负载算法配置错误
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:40:42 [ERROR] - declaration.loadBalancingMode: declaration.loadBalancingMode must be one of the following: "dynamic-ratio-member", "dynamic-ratio-node", "fastest-app-response", "fastest-node", "least-connections-member", "least-connections-node", "least-sessions", "observed-member", "observed-node", "predictive-member", "predictive-node", "ratio-least-connections-member", "ratio-least-connections-node", "ratio-member", "ratio-node", "ratio-session", "round-robin", "weighted-least-connections-member", "weighted-least-connections-node"
----

|少逗号类语法错误
|

[source, bash]
.*CIS 日志*
----
2022/07/26 02:42:36 [ERROR] Error processing configmap cm-211 in namespace: f5-hub-1 with err: invalid character '"' after object key:value pair
----

|POD 扩容
|

[source, bash]
.*LTM 日志*
----
Jul 26 10:18:28 bigip1.com notice mcpd[7242]: 01070727:5: Pool /cistest211/app/app_svc_pool member /cistest211/100.64.23.37:8080 monitor status up. [ /Common/tcp: up ]  [ was unchecked for 0hr:0min:1sec ]
----

|POD 缩容
|

[source, bash]
.*LTM 日志*
----
Jul 26 10:23:41 bigip1.com notice mcpd[7242]: 01070638:5: Pool /cistest211/app/app_svc_pool member /cistest211/100.64.21.153:8080 monitor status down. [ /Common/tcp: down; last error:  ]  [ was up for 0hr:3mins:49sec ]
----

|服务移除
|

[source, bash]
.*LTM 日志*
----
Jul 26 10:23:41 bigip1.com notice mcpd[7242]: 01070638:5: Pool /cistest211/app/app_svc_pool member /cistest211/100.64.21.153:8080 monitor status down. [ /Common/tcp: down; last error:  ]  [ was up for 0hr:3mins:49sec ]
----

|服务恢复
|

[source, bash]
.*CIS 日志* 
----
Jul 26 10:23:41 bigip1.com notice mcpd[7242]: 01070638:5: Pool /cistest211/app/app_svc_pool member /cistest211/100.64.21.153:8080 monitor status down. [ /Common/tcp: down; last error:  ]  [ was up for 0hr:3mins:49sec ]
----

|===

=== 平滑升级测试项

[source, bash]
.*1. 创建应用*
----

----

== 性能测试

=== SVC 在一个 NS 场景(SVC 数量为 150，统计增加/删除/更新 1 个 SVC，同步到 BIG-IP 的时间)

统计增加一个 SVC 下发到 BIG-IP 所需的时间，删除一个 SVC 同步到 BIG-IP 所需的时间，SVC 更新同步到 BIG-IP 所需的时间。

=== SVC 在不同 NS 场景(SVC 数量为 150，统计增加/删除/更新 1 个 SVC，同步到 BIG-IP 的时间)

统计增加一个 SVC 下发到 BIG-IP 所需的时间，删除一个 SVC 同步到 BIG-IP 所需的时间，SVC 更新同步到 BIG-IP 所需的时间。

